<!DOCTYPE html> <html lang=en> <head> <link href="https://fonts.googleapis.com/css?family=Mukta|Poppins:500,600&display=swap" rel=stylesheet> <meta charset=utf-8> <meta content="width=device-width,initial-scale=1" name=viewport> <meta content=#333333 name=theme-color> <base href=/ > <link href=global.css rel=stylesheet> <link href=manifest.json rel=manifest crossorigin=use-credentials> <link href=favicon.png rel=icon type=image/png> <link href=client/main.3898572404.css rel=stylesheet><link href=client/[slug].3f331e1e.css rel=stylesheet><link href=client/client.e75a9efc.css rel=stylesheet><link href=client/Tag.2ddad0a9.css rel=stylesheet> <noscript id=sapper-head-start></noscript><title>üí•Anything2Vec: Mapping Reddit into Vector Spaces - Cameron Raymondüí•</title><link href=https://cameronraymond.me/blog/anything2vec/ rel=canonical><meta content="Word2Vec is a powerful machine learning technique for embedding text corpus' into vector spaces. While useful for NLP problems, this blog post shows how it can also be used to represent and better understand communities on Reddit." name=description><meta content="Cameron Raymond, University of Oxford, Oxford University, Data
    Science, Social Data Sience, Data Scientist" name=keywords><meta content=website property=og:type><meta content=https://cameronraymond.me/blog/anything2vec/ property=og:url><meta content="üí•Anything2Vec: Mapping Reddit into Vector Spaces - Cameron Raymondüí•" property=og:title><meta content="Word2Vec is a powerful machine learning technique for embedding text corpus' into vector spaces. While useful for NLP problems, this blog post shows how it can also be used to represent and better understand communities on Reddit." name=og:description><meta content=https://cameronraymond.me/networkd.png property=og:image><meta content=summary property=twitter:card><meta content=https://cameronraymond.me/blog/anything2vec/ property=twitter:url><meta content="üí•Anything2Vec: Mapping Reddit into Vector Spaces - Cameron Raymondüí•" property=twitter:title><meta content="Word2Vec is a powerful machine learning technique for embedding text corpus' into vector spaces. While useful for NLP problems, this blog post shows how it can also be used to represent and better understand communities on Reddit." property=twitter:description><meta content=https://cameronraymond.me/networkd.png property=twitter:image><noscript id=sapper-head-end></noscript> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-133541363-2"></script> <script> window.dataLayer = window.dataLayer || []
      function gtag() {
        dataLayer.push(arguments)
      }
      gtag('js', new Date())

      gtag('config', 'UA-133541363-2') </script> </head> <body> <div id=sapper> <div class="svelte-ex3z1i margin"></div> <form action=https://tinyletter.com/cjkraymond class="svelte-ex3z1i sign-up-banner" method=post onsubmit="window.open('https://tinyletter.com/cjkraymond', 'popupwindow',
  'scrollbars=yes,width=800,height=600');return true" target=popupwindow><p><label class="svelte-ex3z1i label" for=tlemail>Let's be pals:</label></p> <div class=signup-input><input class="svelte-ex3z1i email" name=email id=tlemail placeholder=my@email.com></div> <input class=svelte-ex3z1i name=embed type=hidden value=1> <span class="svelte-ex3z1i tooltip"><input class="svelte-ex3z1i subscribe" type=submit value=‚úâÔ∏è> <span class="svelte-ex3z1i tooltiptext">Subscribe!</span></span> <span class=svelte-ex3z1i id=close-sub><svg class=svelte-c8tyih viewBox="0 0 512 512" xmlns=http://www.w3.org/2000/svg><path d="M331.3 308.7L278.6 256l52.7-52.7c6.2-6.2 6.2-16.4 0-22.6-6.2-6.2-16.4-6.2-22.6 0L256 233.4l-52.7-52.7c-6.2-6.2-15.6-7.1-22.6 0-7.1 7.1-6 16.6 0 22.6l52.7 52.7-52.7 52.7c-6.7 6.7-6.4 16.3 0 22.6 6.4 6.4 16.4 6.2 22.6 0l52.7-52.7 52.7 52.7c6.2 6.2 16.4 6.2 22.6 0 6.3-6.2 6.3-16.4 0-22.6z"></path> <path d="M256 76c48.1 0 93.3 18.7 127.3 52.7S436 207.9 436 256s-18.7 93.3-52.7 127.3S304.1 436 256 436c-48.1 0-93.3-18.7-127.3-52.7S76 304.1 76 256s18.7-93.3 52.7-127.3S207.9 76 256 76m0-28C141.1 48 48 141.1 48 256s93.1 208 208 208 208-93.1 208-208S370.9 48 256 48z"></path></svg></span></form> <nav class=svelte-1otdzf2><a aria-label=Home href=. class=svelte-1otdzf2 rel=prefetch>Cameron Raymond </a> <div class="svelte-1otdzf2 links"><a aria-label=Resume href=cameron-raymond-resume.pdf class=svelte-1otdzf2 target=_blank><span class="svelte-1otdzf2 icon hideIcons"><svg class=svelte-c8tyih viewBox="0 0 576 512" xmlns=http://www.w3.org/2000/svg><path d="M552 64H88c-13.255 0-24 10.745-24 24v8H24c-13.255 0-24 10.745-24 24v272c0 30.928 25.072 56 56 56h472c26.51 0 48-21.49 48-48V88c0-13.255-10.745-24-24-24zM56 400a8 8 0 0 1-8-8V144h16v248a8 8 0 0 1-8 8zm236-16H140c-6.627 0-12-5.373-12-12v-8c0-6.627 5.373-12 12-12h152c6.627 0 12 5.373 12 12v8c0 6.627-5.373 12-12 12zm208 0H348c-6.627 0-12-5.373-12-12v-8c0-6.627 5.373-12 12-12h152c6.627 0 12 5.373 12 12v8c0 6.627-5.373 12-12 12zm-208-96H140c-6.627 0-12-5.373-12-12v-8c0-6.627 5.373-12 12-12h152c6.627 0 12 5.373 12 12v8c0 6.627-5.373 12-12 12zm208 0H348c-6.627 0-12-5.373-12-12v-8c0-6.627 5.373-12 12-12h152c6.627 0 12 5.373 12 12v8c0 6.627-5.373 12-12 12zm0-96H140c-6.627 0-12-5.373-12-12v-40c0-6.627 5.373-12 12-12h360c6.627 0 12 5.373 12 12v40c0 6.627-5.373 12-12 12z"></path></svg></span> <span class="svelte-1otdzf2 hideLinks">Resume</span></a> <a aria-label=Blog href=blog/ class=svelte-1otdzf2 rel=prefetch aria-current=page><span class="svelte-1otdzf2 icon hideIcons"><svg class=svelte-c8tyih viewBox="0 0 512 512" xmlns=http://www.w3.org/2000/svg><path d="M497.9 142.1l-46.1 46.1c-4.7 4.7-12.3 4.7-17 0l-111-111c-4.7-4.7-4.7-12.3 0-17l46.1-46.1c18.7-18.7 49.1-18.7 67.9 0l60.1 60.1c18.8 18.7 18.8 49.1 0 67.9zM284.2 99.8L21.6 362.4.4 483.9c-2.9 16.4 11.4 30.6 27.8 27.8l121.5-21.3 262.6-262.6c4.7-4.7 4.7-12.3 0-17l-111-111c-4.8-4.7-12.4-4.7-17.1 0zM124.1 339.9c-5.5-5.5-5.5-14.3 0-19.8l154-154c5.5-5.5 14.3-5.5 19.8 0s5.5 14.3 0 19.8l-154 154c-5.5 5.5-14.3 5.5-19.8 0zM88 424h48v36.3l-64.5 11.3-31.1-31.1L51.7 376H88v48z"></path></svg></span> <span class="svelte-1otdzf2 hideLinks">Blog</span></a> <a aria-label=About href=about/ class=svelte-1otdzf2 rel=prefetch><span class="svelte-1otdzf2 icon hideIcons"><svg class=svelte-c8tyih viewBox="0 0 448 512" xmlns=http://www.w3.org/2000/svg><path d="M224 256c70.7 0 128-57.3 128-128S294.7 0 224 0 96 57.3 96 128s57.3 128 128 128zm89.6 32h-16.7c-22.2 10.2-46.9 16-72.9 16s-50.6-5.8-72.9-16h-16.7C60.2 288 0 348.2 0 422.4V464c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48v-41.6c0-74.2-60.2-134.4-134.4-134.4z"></path></svg></span> <span class="svelte-1otdzf2 hideLinks">About</span></a> <div class="svelte-1otdzf2 divider"></div> <a aria-label=LinkedIn href=https://www.linkedin.com/in/CJKRaymond/ class="svelte-1otdzf2 icon"><svg class=svelte-c8tyih viewBox="0 0 448 512" xmlns=http://www.w3.org/2000/svg><path d="M100.28 448H7.4V148.9h92.88zM53.79 108.1C24.09 108.1 0 83.5 0 53.8a53.79 53.79 0 0 1 107.58 0c0 29.7-24.1 54.3-53.79 54.3zM447.9 448h-92.68V302.4c0-34.7-.7-79.2-48.29-79.2-48.29 0-55.69 37.7-55.69 76.7V448h-92.78V148.9h89.08v40.8h1.3c12.4-23.5 42.69-48.3 87.88-48.3 94 0 111.28 61.9 111.28 142.3V448z"></path></svg></a> <a aria-label=twitter href=https://twitter.com/CJKRaymond class="svelte-1otdzf2 icon"><svg class=svelte-c8tyih viewBox="0 0 512 512" xmlns=http://www.w3.org/2000/svg><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a></div></nav> <main class=svelte-y41jpk> <div></div> </main> <div class="svelte-16irf4m chat"><div></div></div> <div class="svelte-16irf4m footer"><p class=svelte-16irf4m><span aria-hidden=false aria-label=Emoji role=img>üë®‚Äçüé®</span> + <span aria-hidden=false aria-label=Emoji role=img>üë∑‚Äç‚ôÇÔ∏è</span> by me <br> Last updated <a aria-label="April
      06, 21" href=https://www.onthisday.com/events/April/06>April 06, '21</a></p> <div class="svelte-16irf4m links"><a aria-label=LinkedIn href=https://www.linkedin.com/in/CJKRaymond/ class="svelte-16irf4m icon"><svg class=svelte-c8tyih viewBox="0 0 448 512" xmlns=http://www.w3.org/2000/svg><path d="M100.28 448H7.4V148.9h92.88zM53.79 108.1C24.09 108.1 0 83.5 0 53.8a53.79 53.79 0 0 1 107.58 0c0 29.7-24.1 54.3-53.79 54.3zM447.9 448h-92.68V302.4c0-34.7-.7-79.2-48.29-79.2-48.29 0-55.69 37.7-55.69 76.7V448h-92.78V148.9h89.08v40.8h1.3c12.4-23.5 42.69-48.3 87.88-48.3 94 0 111.28 61.9 111.28 142.3V448z"></path></svg></a> <div class="svelte-16irf4m divider"></div> <a aria-label=twitter href=https://twitter.com/CJKRaymond class="svelte-16irf4m icon"><svg class=svelte-c8tyih viewBox="0 0 512 512" xmlns=http://www.w3.org/2000/svg><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a> <div class="svelte-16irf4m divider"></div> <a aria-label=Github href=https://github.com/cameron-raymond/ class="svelte-16irf4m icon"><svg class=svelte-c8tyih viewBox="0 0 496 512" xmlns=http://www.w3.org/2000/svg><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a> <div class="svelte-16irf4m divider"></div> <a aria-label=Medium href=https://medium.com/@cameronraymond/ class="svelte-16irf4m icon"><svg class=svelte-c8tyih viewBox="0 0 512 512" xmlns=http://www.w3.org/2000/svg><path d="M71.5 142.3c.6-5.9-1.7-11.8-6.1-15.8L20.3 72.1V64h140.2l108.4 237.7L364.2 64h133.7v8.1l-38.6 37c-3.3 2.5-5 6.7-4.3 10.8v272c-.7 4.1 1 8.3 4.3 10.8l37.7 37v8.1H307.3v-8.1l39.1-37.9c3.8-3.8 3.8-5 3.8-10.8V171.2L241.5 447.1h-14.7L100.4 171.2v184.9c-1.1 7.8 1.5 15.6 7 21.2l50.8 61.6v8.1h-144v-8L65 377.3c5.4-5.6 7.9-13.5 6.5-21.2V142.3z"></path></svg></a></div></div> <a aria-label=" " href=blog/ class=svelte-1i4fw52>blog</a> <a aria-label=" " href=sitemap.xml class=svelte-1i4fw52>sitemap</a></div> <script>__SAPPER__={baseUrl:"",preloaded:[void 0,null,{post:{title:"Anything2Vec: Mapping Reddit into Vector Spaces",slug:"anything2vec",emoji:"üí•",blurb:"Word2Vec is a powerful machine learning technique for embedding text corpus' into vector spaces. While useful for NLP problems, this blog post shows how it can also be used to represent and better understand communities on Reddit.",type:"bp",tags:["ml"],link:"\u003Ca aria-label='Blog' href='https:\u002F\u002Fmedium.com\u002F@cameronraymond\u002Fanything2vec-mapping-reddit-into-vector-spaces-dcc77d9f3bea'\u003EBlog\u003C\u002Fa\u003E",date:"Jul. 2, 2020",prod:true,html:"\u003Cimg src=\"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F3844\u002F1*_FvvzFGBcS3eb5eXlKSMgg.png\" alt=\"‚ÄúSubreddit Embedding‚Äù and the 100 closest subreddits to \u002Fr\u002Fnba\"\u003E\u003Cem\u003E‚ÄúSubreddit Embedding‚Äù and the 100 closest subreddits to \u002Fr\u002Fnba\u003C\u002Fem\u003E\n\u003Cp\u003EA common problem in ML, natural language processing (NLP), and AI at large surrounds representing objects in a way computers can process. And since computers understand numbers ‚Äî  which we have a common language for comparing, combining and manipulating ‚Äî this generally means assigning objects numbers in some fashion. Think taking something abstract but intuitive to humans, like the text of a book, and assigning each word in that book a unique number. That book could then be represented by the list, or vector, of numbers assigned to it. This is the process \u003Cem\u003Eof embedding that book as a vector ‚Äî\u003C\u002Fem\u003E and there is an increasingly rich literature of techniques for embedding objects as vectors.\u003C\u002Fp\u003E\u003Cp\u003EWhile much of this literature focuses on representing \u003Cem\u003Ewords\u003C\u002Fem\u003E as vectors, which can aide in NLP problems, much of the logic can be transferred to embedding any arbitrary set of objects. Through my research at the University of Toronto, and their computational social science lab, I‚Äôve been applying embedding techniques to understand online forums like Reddit. This article is meant to serve as a starting point to break down the research that is being done at UofT. For more information on my research check out \u003Ca href=\"https:\u002F\u002Fcameronraymond.me\"\u003Ehttps:\u002F\u002Fcameronraymond.me\u003C\u002Fa\u003E, and for the original paper that this article is based on see \u003Ca href=\"https:\u002F\u002Fdl.acm.org\u002Fdoi\u002Fabs\u002F10.1145\u002F3308558.3313729\"\u003EWaller, I., &amp; Anderson, A\u003C\u002Fa\u003E.\u003C\u002Fp\u003E\u003Cp\u003EFirst, we‚Äôll take a look at what it means to embed some \u003Cem\u003Ething\u003C\u002Fem\u003E as a vector and what a good embedding entails. Then we‚Äôll take a common embedding technique, Word2Vec, and see how it is used to model words as vectors. After seeing why Word2Vec is so useful, we can start to generalize its principles and show its utility in mapping the different communities of Reddit.\u003C\u002Fp\u003E\u003Ch2 id=\"what-is-an-embedding\"\u003EWhat is an embedding?\u003C\u002Fh2\u003E\n\u003Cp\u003EWhile embedding techniques can get complex ‚Äî  at its core, to embed some \u003Cem\u003Ething\u003C\u002Fem\u003E is just to represent that \u003Cem\u003Ething\u003C\u002Fem\u003E as a vector of real numbers. This is useful because there‚Äôs a common currency when talking about vectors of real numbers; namely they are easy to  add, subtract, compare and manipulate. So to embed some \u003Cem\u003Eset\u003C\u002Fem\u003E of objects then is just to represent those objects with \u003Cem\u003Eunique\u003C\u002Fem\u003E vectors of real numbers. So not all embedding techniques involve complex neural nets, and often simple embeddings are powerful enough for a given problem; however, there are benefits to  more nuanced techniques that we‚Äôll focus on.\u003C\u002Fp\u003E\u003Cp\u003EA ‚Äòdumb embedding‚Äô would be to one-hot encode \u003Cem\u003Eall the different unique objects\u003C\u002Fem\u003E as their own unit basis vector. This means that in a set of |\u003Cem\u003EV\u003C\u002Fem\u003E| objects, each object in that set, \u003Cem\u003Ev\u003C\u002Fem\u003E, is represented as a vector of size |\u003Cem\u003EV\u003C\u002Fem\u003E| with all 0s, except for the \u003Cem\u003Evth\u003C\u002Fem\u003E index which is a 1.\u003C\u002Fp\u003E\u003Cimg src=\"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F2000\u002F1*UOjWvDziH86T2MmiDpp98Q.png\" alt=\"One-hot encoding of the words: red, yellow and green. Source: [Kaggle](https:\u002F\u002Fwww.kaggle.com\u002Fdansbecker\u002Fusing-categorical-data-with-one-hot-encoding).\"\u003E\u003Cem\u003EOne-hot encoding of the words: red, yellow and green. Source: \u003Ca href=\"https:\u002F\u002Fwww.kaggle.com\u002Fdansbecker\u002Fusing-categorical-data-with-one-hot-encoding\"\u003EKaggle\u003C\u002Fa\u003E.\u003C\u002Fem\u003E\n\u003Cp\u003EWhy might this not be a powerful enough embedding? Even though we have the tools to manipulate these vectors, it may not return intuitive results. This is because when objects are one-hot encoded, the embedding isn‚Äôt tied back to the real world in anyway. Specifically, there isn‚Äôt a logical relationship between objects‚Äô representations that reflects their actual relationships; each vector is equally far from every other vector. In an ideal world, you may want the vector representing ‚Äòred‚Äô \u003Cem\u003E([red]=&lt;1 0 0&gt;)\u003C\u002Fem\u003E and the vector representing ‚Äòyellow‚Äô \u003Cem\u003E([yellow]=&lt;0 1 0&gt;),\u003C\u002Fem\u003E when added together, to return the vector representing ‚Äòorange‚Äô \u003Cem\u003E([orange]-&gt;&lt;1 1 0&gt;).\u003C\u002Fem\u003E One-hot encoding only lets you say what an item is by its vector, it doesn‚Äôt tell you how the vectors relate to one another. With that said, one-hot encoding is often a good starting point.\u003C\u002Fp\u003E\u003Cp\u003ETo understand how we can embed objects in a way \u003Cem\u003Ethat is tied back to the real world\u003C\u002Fem\u003E we‚Äôll look at a more nuanced technique called Word2Vec. While generally used to embed words, it generalizes to arbitrary objects in certain cases as well. Word2Vec allows us to represent each object from a set of objects as a \u003Cem\u003Edense vector of real numbers\u003C\u002Fem\u003E in a way that preserves relations between different objects.\u003C\u002Fp\u003E\u003Cp\u003ETo get the intuition behind how Word2Vec works, we‚Äôll look at its most common use case: embedding words as vectors. As such, those familiar with Word2Vec can skip the next section. From there we‚Äôll see how Word2Vec can generalize to embed other objects. For this we‚Äôll embed Reddit‚Äôs 10,000 most active communities. Finally, we‚Äôll show how this embedding aligns with our understanding of what these communities represent.\u003C\u002Fp\u003E\u003Ch2 id=\"word2vec\"\u003EWord2Vec\u003C\u002Fh2\u003E\n\u003Cp\u003EThe underlying intuition behind Word2Vec is that two words are similar if they are used in similar ways. For example if you substitute the word ‚Äògood‚Äô for the word ‚Äògreat‚Äô in a sentence, it will likely still make sense. This concept is well summarized by the linguist John Rupert Firth who, in 1957, said ‚Äúyou shall know a word by the company it keeps.‚Äù While there are various implementations of Word2Vec, this article will focus on the Skip-gram model which fits in well with Firth‚Äôs ideas.\u003C\u002Fp\u003E\u003Cblockquote\u003E\n\u003Ch3 id=\"you-shall-know-a-word-by-the-company-it-keeps--jr-firth\"\u003E‚ÄúYou shall know a word by the company it keeps.‚Äù ‚Äî J.R. Firth\u003C\u002Fh3\u003E\n\u003C\u002Fblockquote\u003E\n\u003Cp\u003EThe Skip-gram model ‚Äî when applied to words ‚Äî \u003Cstrong\u003Egoes through each word in the text corpus and tries to predict the \u003Cem\u003En\u003C\u002Fem\u003E words on either side of it.\u003C\u002Fstrong\u003E The \u003Cem\u003En\u003C\u002Fem\u003E words surrounding the target word are its context. In the picture below we see that the context for the word ‚Äònasty‚Äô are ferocious, dog‚Äôs, sharp and bite.\u003C\u002Fp\u003E\u003Cimg src=\"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F2000\u002F1*jg-Tx3IpkpkTy-bQeM5Z_w.png\" alt=\"Word-Traget Relation\"\u003E\n\u003Cp\u003EWe start off by one-hot encoding each word, and then use a \u003Cem\u003Eshallow neural network\u003C\u002Fem\u003E to predict all the context vectors associated with the target word. In this way, words used in similar contexts will have similar output vectors. By taking the output of the hidden layer, before converting the output into the concatenation of the one-hot encoded vectors, we can represent that word as a dense vector of real numbers.\u003C\u002Fp\u003E\u003Cimg src=\"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F2000\u002F1*s__GyNO0aw2C_EFZie8keg.png\" alt=\"Word2Vec Skip-gram model\"\u003E\n\u003Cp\u003EThrough this training process Word2Vec preserves semantic as well as syntactic shifts in language. For example the transformation from the vector representing the word ‚ÄòKing‚Äô (denoted by \u003Cem\u003E[King]\u003C\u002Fem\u003E ) to \u003Cem\u003E[Queen]\u003C\u002Fem\u003E is roughly the same as the transformation from \u003Cem\u003E[Man]\u003C\u002Fem\u003E to \u003Cem\u003E[Woman].\u003C\u002Fem\u003E Therefore we can represent the analogy \u003Cem\u003EMan is to Woman as King is to Queen\u003C\u002Fem\u003E as \u003Cem\u003E[Man]-[Woman]=[King]-[Queen].\u003C\u002Fem\u003E And if we didn‚Äôt already know that Queen is the final component of the analogy, we could solve for it using the equation \u003Cem\u003E[Queen] = [King]-[Man]+[Woman].\u003C\u002Fem\u003E\u003C\u002Fp\u003E\u003Cimg src=\"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F2800\u002F1*wohQJmOwOmPR0v0eSv_5sA.png\" alt=\"Embedding Analogy Examples\"\u003E\n\u003Ch2 id=\"anything2vec\"\u003EAnything2Vec\u003C\u002Fh2\u003E\n\u003Cp\u003EThe Skip-gram model has been well explored when applied to words, as seen through the popularity of Word2Vec, but its utility doesn‚Äôt stop at linguistic analogies. For this we‚Äôll show how Word2Vec generalizes to situations where there‚Äôs a logical target-context relation.\u003C\u002Fp\u003E\u003Ch3 id=\"subreddit-embeddings\"\u003ESubreddit Embeddings\u003C\u002Fh3\u003E\n\u003Cp\u003EJust as you can ‚Äúknow a word by the company it keeps,‚Äù the same logic applies to Reddit and its variety of online communities, called subreddits. The, less pithy, analog in this case is that we can know a subreddit by the commenters it keeps. For the Skip-gram model, each subreddit represents a ‚Äúword‚Äù and that subreddit‚Äôs commenters act as the ‚Äúcontext.‚Äù So like Word2Vec, subreddits with similar commenters will have similar output vectors.\u003C\u002Fp\u003E\u003Cimg src=\"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F2000\u002F1*lq_g8iPDNgpI_6M2vOGmyw.png\" alt=\"Subreddit Skip-gram model\"\u003E\n\u003Cp\u003EWhile the output vectors are embedded in a high dimensional vector space (often 150+ dimensions), and thus can‚Äôt be visualized, \u003Ca href=\"https:\u002F\u002Fstats.stackexchange.com\u002Fquestions\u002F2691\u002Fmaking-sense-of-principal-component-analysis-eigenvectors-eigenvalues\"\u003Eprincipal component analysis\u003C\u002Fa\u003E can return a 3-dimensional approximation. Below is a visualization of such an approximation for all 10,000 subreddits. In this plot we‚Äôve highlighted the hip hop oriented subreddit, \u003Cem\u003E\u002Fr\u002Fhiphopheads,\u003C\u002Fem\u003E and it‚Äôs 100 closest vectors. As we can see, the closest subreddits by cosine similarity are also hip hop themed.\u003C\u002Fp\u003E\u003Cimg src=\"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F4034\u002F1*RQd35n1X61pCJZOmRZibPg.png\" alt=\"Subreddit embedding\"\u003E\n\u003Ch3 id=\"subreddit-analogies\"\u003ESubreddit Analogies\u003C\u002Fh3\u003E\n\u003Cp\u003EWith Word2Vec, the resulting embeddings can preserve relationships between words. This allows for simple vector addition and subtraction to answer analogy problems. For example, to answer the analogy \u003Cem\u003EBerlin is to Germany as Ottawa is to x,\u003C\u002Fem\u003E we calculate \u003Cem\u003E[x]=[Germany]-[Berlin]+[Ottawa]\u003C\u002Fem\u003E and choose the closest vector to \u003Cem\u003E[x]\u003C\u002Fem\u003E which would be \u003Cem\u003E[Canada].\u003C\u002Fem\u003E This property holds for our subreddit embedding as well. When posing the analogy \u003Cem\u003E\u002Fr\u002Fboston is to \u002Fr\u002Fchicago as \u002Fr\u002Fbostonceltics is to x,\u003C\u002Fem\u003E the closest vector to \u003Cem\u003E[\u002Fr\u002Fbostonceltics]-[\u002Fr\u002Fboston]+[\u002Fr\u002Fchicago]\u003C\u002Fem\u003E is the subreddit dedicated to the Chicago Bulls.\u003C\u002Fp\u003E\u003Cimg src=\"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F2000\u002F1*XfmkMyd5FVJ5vxZGAla0og.png\" alt=\"Vector transformation from a city to its corresponding NBA team.\"\u003E\u003Cem\u003EVector transformation from a city to its corresponding NBA team.\u003C\u002Fem\u003E\n\u003Cp\u003EOn a testing set of ~1,500 similar analogy problems (city to sports team, university to university town, state to state capital) our embedding attained 81% accuracy.\u003C\u002Fp\u003E\u003Ch2 id=\"when-and-when-not\"\u003EWhen and When Not?\u003C\u002Fh2\u003E\n\u003Cp\u003EThe core intuition behind Word2Vec, and its generalization, is that you can represent words, subreddits, Twitter users, \u003Cem\u003Eetc‚Ä¶\u003C\u002Fem\u003E by the company they keep. Words used in similar contexts are likely similar; the same holds for  subreddits with similar commenters and Twitter users with similar followers. However, if there isn‚Äôt enough data, the embedding isn‚Äôt likely to pick up on the different dimensions in which the entities can be similar or different. Any user on Reddit likely comments on a variety of subreddits, not all of which are related. Yet, from a macro point of view, over millions of comments, very nuanced relations begin to emerge.\u003C\u002Fp\u003E\u003Cp\u003EBy first starting with a bare-bones approach to what an embedding can be, and then seeing how more nuanced embeddings can improve NLP problems ‚Äî this article showed how embedding techniques can derive interesting results when applied to arbitrary objects, like subreddits. If you have thoughts on how you‚Äôd like to see this work used, feel free to let me know below!\u003C\u002Fp\u003E"}}]};if('serviceWorker' in navigator)navigator.serviceWorker.register('/service-worker.js');(function(){try{eval("async function x(){}");var main="/client/client.e75a9efc.js"}catch(e){main="/client/legacy/client.e53be589.js"};var s=document.createElement("script");try{new Function("if(0)import('')")();s.src=main;s.type="module";s.crossOrigin="use-credentials";}catch(e){s.src="/client/shimport@1.0.1.js";s.setAttribute("data-main",main);}document.head.appendChild(s);}());</script> 