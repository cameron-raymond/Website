{"title":"Do Neural Networks EverÂ Forget?","slug":"right-to-be-forgotten-ml","emoji":"ğŸ§ ","blurb":"How machine learning throws a wrench in the 'right to be forgotten.' Bringing in some of the latest computational research on privacy, this post examines how the principles of GDPR collide with the realities of neural networks.","type":"bp","tags":["pl"],"link":"<a aria-label='Blog' href='https://medium.com/@cameronraymond/5046530eb844?source=friends_link&sk=dbfc72d8c7f3173d8496d3b5ab0e3243'>Blog</a>","date":"Jun. 4, 2020","prod":true,"html":"<img src=\"https://cdn-images-1.medium.com/max/2000/1*pmSFu6KWJGdl3ZJcA_6Q8A.png\" alt=\"Unintended Feature Leakage from Gender Classification. Source: [Melis, Luca, et al.](https://ieeexplore.ieee.org/abstract/document/8835269/?casa_token=xWJF2Qn5p04AAAAA:8onczj50twpsKTaybecxy-CIAIgSRoWJ5NeJ9p0hMw53pP3t5JHJjkjpeF7wd4FLRZzd9XgnoFw)\"><em>Unintended Feature Leakage from Gender Classification. Source: <a href=\"https://ieeexplore.ieee.org/abstract/document/8835269/?casa_token=xWJF2Qn5p04AAAAA:8onczj50twpsKTaybecxy-CIAIgSRoWJ5NeJ9p0hMw53pP3t5JHJjkjpeF7wd4FLRZzd9XgnoFw\">Melis, Luca, et al.</a></em>\n<p>As the usage of data evolves, so should its regulation. Faster and faster, the digital world is embedding itself in our lives to remove friction. Tech removes friction by learning about us and how we behave as a collective, anticipating and reacting accordingly.  Think Starbucks sending you a push notification whenever you come close to one of their stores â€” one ad for a latte if itâ€™s cold out, one for an iced coffee if itâ€™s hot. This has made firms like Facebook, Amazon, Apple, Netflix, and Google some of the most valuable (<em>the most valuable</em>, bar none, if you consider how few employees they have) in history, giving them an out-sized influence on our lives. So it is important to ask: who are these firms accountable to? Or more importantly, what are the market forces that affect how we, their users, are treated? Facebookâ€™s misuse of data with Cambridge Analytica, and Googleâ€™s <a href=\"https://www.wired.com/2012/05/google-wifi-fcc-investigation/\">rogue engineer</a> who adapted a fleet of Street View cars to siphon often-sensitive data from private WiFi networks, have lead to reasonable concerns surrounding how much regulation is needed in tech. Unfortunately, when it comes to protecting our data, privacy legislation fails to take into account artificial intelligence (AI). Instead legislation, like the EUâ€™s General Data Protection Regulation (GDPR), focuses on the explicit collection and transfer of personal information. <strong>This ignores what makes data useful to tech firms, how it can be generalized and modeled to commodify everyday behaviour.</strong> In this way, machine learning (ML) undermines traditional privacy legislation in twice over: it complicates an our right to access and appeal how organizations use our personal information, and it ignores how ML makes implicit use of personal data.</p><p>This argument is a little more nuanced than pointing out the consequences of a world where training data can be reverse engineered, though, this is also a concern. Instead, I want to focus on what privacy legislature attempts to protect: our ability to know how companies use our data, and our ability to maintain control of our data. In doing so weâ€™ll see that ML makes it harder to interrogate how companies  use our data. Weâ€™ll also see that correcting how our data is used in these systems is much harder than correcting the data that is protected in GDPR. Finally, Iâ€™ll make the argument that if our aim is to give greater control over how our data is used, then the right to be forgotten must also apply to ML. Otherwise, we will be ignoring what Shoshana Zuboff calls techâ€™s new â€œlogic of accumulation.â€Â² If you arenâ€™t a fan of Zuboff, or the term â€œlogic of accumulationâ€ is foreign or off putting, hold onâ€” thatâ€™s where weâ€™ll start. To cap things off Iâ€™ll put a spotlight on some of the latest research that aims to address these problems.</p><h2 id=\"what-makes-our-data-useful\">What Makes Our Data Useful?</h2>\n<p>Before we can understand how GDPR fails to protect <em>the use of our data</em>, we need a better understanding of what the connection is between tech firms and our personal privacy. The rapid rise of connectivity and proliferation of uses of the internet has brought about what Shoshana Zuboff considers a new technological logic of accumulation, where big data â€œorganizes perception and shapes the expression of technological affordances at their roots.â€Â² This is an overly academic way of saying that <em>big data has changed how we view the world â€” and as a result the way firms, like Google, operate is fundamentally different from non-data oriented  firms.</em> Private organizations are able to gain a deep knowledge of our online interactions â€œfrom aboveâ€, anonymously monitoring everyday behaviour to model and exploit whatever information they can gleamÂ². Through continuous data mining and analysis, the Googleâ€™s and Facebookâ€™s of our world are able to understand how we behave at a tremendously granular levelâ¸. The digital bread crumbs we leave behind are collected, stored and then aggregated and modeled to better target, personalize, and enforce. This is what researchers refer to as â€œthe commodification of everyday behaviour.â€Â² Tech act as indifferent observers who spread their â€œfreeâ€ products as widely as possible, to model our behaviour for the benefit of advertisers, insurers, <em>etc.</em> This digital-/data-first process has produced relatively small firms, with fewer fixed costs, that generate tremendous amounts of wealth. And thanks to the unique corporate structures of <a href=\"https://www.vox.com/technology/2018/11/19/18099011/mark-zuckerberg-facebook-stock-nyt-wsj\">Facebook</a> and Google, the ability to leverage those assets are often directed by one or two people.</p><p>The onus lies on policy makers to ensure that technologyâ€™s advances are brought about in an equitable way. A holistic privacy policy is necessary; legislation must allow for the fair, transparent collection of data, as well as ensure that data are processed and utilized in an equitable way. <strong>While tech firms require near ubiquitous monitoring to produce the lakes of data it feeds off of, their  true value come from the ability to process and make data useful.</strong> Given the enormity of data, this  is only made possible through ML. Theoretical and practical advances in ML let tech firms search, sort, cluster and make decisions based off subterranean patterns in data. Therefore, collection and utilization are inextricably linked. However, this is not how our privacy legislation has viewed data collection. Instead, policymakers have generally focused on the former, without recognizing how data collected is exploited implicitly in its utilization.</p><h2 id=\"what-gdpr-does-and-doesnt-do\">What GDPR Does (and Doesnâ€™t) Do</h2>\n<p>Private firms â€” leveraging largely public datasets -â€“ fundamentally altered the United Kingdomâ€™s referendum to leave the EU, and the 2016 US election of Donald TrumpÂ³. In my opinion these major events are what brought questions about how our data is used into the public consciousness. Between the weekâ€™s starting April 10, 2016 and April 10, 2019, Google search interest saw increases of 119%â´, 1,566%âµ and 81%â¶ for the search terms: data privacy, AI ethics, and privacy software respectively. In this same timeframe, Google search interest surrounding artificial intelligence and machine learning also saw a steep uptick with corresponding 43% and 200% increasesâ·. So it is not surprising that the largest piece of privacy legislation born in this political landscape, the EUâ€™s 2016 GDPR, has been the subject of popular debate and scrutiny. <strong>GDPR regulates the processing and free movement of data, and affords individuals the â€œprotection of [their] personal dataâ€ through three core sections: the right to informed consent, the right to access personal data, and the right to rectification and erasure</strong>â¸. GDPR gives increased protections to individuals, letting you appeal decisions made by autonomous systems. Unfortunately, by focusing on the explicit collection and movement of data it falls prey to similar flaws in earlier pieces of privacy legislature like Canadaâ€™s PIPEDAâ¹. These flaws, which weâ€™ll go into depth on, are that it can be very hard to: access our data after itâ€™s been processed to train a neural net as well as appeal its uses given the often opaque nature of ML. As well, the right to rectification and erasure fails to take into account ML is structured by the data itâ€™s trained on. This allows companies to profit off of our data long after weâ€™ve requested it to be erased.</p><h3 id=\"how-do-you-fight-an-algorithm\">How do you fight an algorithm?</h3>\n<p>GDPR (Article 16 specificallyğŸ˜‰) gives us the right to appeal inaccurate collection or use of our personal data. But while the â€œpurposes of processingâ€ must be taken into account when rectifying inaccurate uses of data, GDPR fails to establish a litmus test for what constitutes inaccurate usageâ¸. Knowing what needs to be fixed is much easier when the data in question relates to some  concrete characteristic. Itâ€™s easy to fix someones name or birthday in a database. However, in cases where an ML system makes some decision about us â€”  like inferring our political orientation, sexuality, or <a href=\"https://www.wired.com/story/crime-predicting-algorithms-may-not-outperform-untrained-humans/\">risk of recidivism</a>â€” how can you appeal to a neural network? This is key because <strong>the data that ML is trained on are produced in an unjust world, and there is often little reason to believe that such models will do anything but replicate preexisting inequalities</strong>Â¹â°. It is what researchers often refer to as algorithmic bias (which is different from statistical bias). This was the case when researchers from Microsoft and Boston University demonstrated that word embeddings can exhibit gender stereotypes to disturbing extentsÂ¹â°. However since even supervised ML is left to its own devices to figure out how to best approximate some regression/classification function, it is less straightforward to argue that you have been discriminated againstÂ¹Â¹. By placing the burden on individuals to meet this vague standard for what inaccurate usage may mean, GDPR ignores the structural biases that are easily replicated and amplified in MLÂ¹Â¹.</p><h3 id=\"do-neural-networks-ever-forget\">Do Neural Networks Ever Forget?</h3>\n<p>GDPR deviates from previous attempts at privacy legislature by giving individuals the â€œright to be forgotten.â€ This means that if you make a request to a company that has possession of your data, they are obligated to erase it. <strong>However, this doesnâ€™t extend to the ML which your data has been trained on.</strong> This is because GDPR fundamentally views data as an input to a machine that makes some decision, when actually, data shapes the decision making system itself. To me, by allowing companies to continuously profit off of our data regardless of individual preferences, this represents a fundamental flaw that ignores techâ€™s logic of accumulation.</p><p>GDPR (Articles 17 through 20 nowğŸ’ƒ) doesnâ€™t recognize that if your data has  been used to train a neural network, you are forever imprinted on itÂ¹Â². Even if you submit an erasure request, and your information doesnâ€™t appear in any of Facebookâ€™s databases, your information is still implicitly being processed when Facebook decides what ad to show someone. This is what brings us back to our header image. Researchers at Cornell, UCL and the Alan Turing Institute recently demonstrated that collaborative learning models can â€œleak <em>unintended</em> information about participantsâ€™ training data,â€ allowing malign actors to â€œinfer the presence of exact data pointsâ€”for example, specific locations [â€¦ as well as] <em>properties</em> that hold only for a subset of the training data and are independent of the properties that the joint model aims to capture.â€Â¹ This, hopefully, drives home the fact that ML is not separate from us, and there is a growing body of literature that argues our data shapes the fundamental structure of these models. In some cases, this literally means adding/dropping nodes from the layers of an ANNÂ¹Â³. In framing erasure in such concrete terms, GDPR fails to remedy techâ€™s more exploitative characteristics and refuses to acknowledge the true utility of data: that it â€œrecords, modifies, and commodifies everyday experience.â€Â²</p><h2 id=\"improving-the-right-to-be-forgotten\">Improving the â€˜right to be forgottenâ€™</h2>\n<p>GDPR gives us the right to challenge companies when they use ML to make decisions about us (what price to give, whether to insure, risk of recidivism). This is a huge step forward. Unfortunately, MLâ€™s quality is to disappear into the background, embedding itself in our digital world. That is to say, there is rarely a big sign saying: â€œWatch out! A neural network is deciding whether youâ€™re too risky to insure!â€ Given the embedded nature of ML, its implementation can subtly shape the online world in ways that, while technically consensual, individuals are not fully aware of. This then puts the onus on individuals to parse out their online world for inaccurate or biased systems in ways that could be far from feasible. It also ensures that only those who have the means to educate themselves on how tech/ML operates will be able to have full control over their data. Over the past sixteen years, there has been surprisingly little to fully address the issue of clear and informed consent.</p><p>The most overlooked aspect of privacy legislature is that there are no protections to let individuals remove themselves from models that infer from user dataÂ¹â´. GDPR does not address the consequences of allowing tech to profit off of models, trained on our data, after weâ€™ve invoked our â€œright to be forgotten.â€ This requires a conceptual shift in how privacy is viewed. The core of tech firms are their ability to cheaply capture data, the raw material, and model it to achieve various ends. <strong>Privacy legislation cannot stop at the collection of data and then interpret the neural net from which it was built as something wholly different.</strong> Privacy legislation should extend to ML as well. As of now this problem is only addressed superficially in GDPR. Thankfully, researchers at the University of  Cambridge and Queen Mary University of London, among others, are proposing technical solutions to these problems. Shintre et al. proposed a novel solution that allows individual data points to be removed from artificial neural networks in their 2019 paper, <em>Making Machine Learning Forget</em>Â¹âµ. What this demonstrates is that there are few technical obstacles to fully realizing systems where we can truly have the right to be forgotten. First however, there must be an understanding of how our data can be used and misused, and the political will to hold tech accountable.</p><h2 id=\"moving-forward\">Moving Forward</h2>\n<p>Technology and ML have undoubtedly made our lives better. However, that doesnâ€™t mean we shouldnâ€™t be critical when tech firms unnecessarily impinge on our rights. Bezos would still be rich if we addressed these issues. In the past 20 years, the tech industry has accumulated massive amounts of user data which legislatures have subsequently had to grapple with. ML undermines the existing forms of privacy legislature in two ways. It subverts the grounds from which we can appeal inaccurate or biased uses of our personal information. As well,  problems arise when users are afforded the right to erasure without acknowledging the embedded nature of data in ML. As a result, We need a conceptual shift in how we view privacy. </p><p>Our data isnâ€™t useful by itself. Given that fact, we need to focus less on the explicit collection and transfer of data, and instead focus more on how our data is used. Our data leaves fingerprints on the neural networks theyâ€™re trained on. Itâ€™s important to remember that those fingerprints are ours as well, and as a result the right to be forgotten should extend to ML.</p><p>[1]: Melis, Luca, Congzheng Song, Emiliano De Cristofaro, and Vitaly Shmatikov. â€œExploiting unintended feature leakage in collaborative learning.â€ In <em>2019 IEEE Symposium on Security and Privacy (SP)</em>, pp. 691â€“706. IEEE, 2019.</p><p>[2]: Zuboff, Shoshana. â€œBig other: surveillance capitalism and the prospects of an information civilization.â€ <em>Journal of Information Technology</em> 30, no. 1 (2015): 75â€“89.</p><p>[3]: Isaak, Jim, and Mina J. Hanna. â€œUser data privacy: Facebook, Cambridge Analytica, and privacy protection.â€ <em>Computer</em> 51, no. 8 (2018): 57.</p><p>[4]: Google Trends, â€œData Privacy Search Interest (2016â€“2019).â€ Accessed on April 10, 2020. <a href=\"https://trends.google.com/trends/explore?date=2016-04-10%202020-04-10&amp;q=Data%20Privacy\">https://trends.google.com/trends/explore?date=2016-04-10%202020-04-10&amp;q=Data%20Privacy</a>.</p><p>[5]: Google Trends, â€œAI Ethics Search Interest (2016â€“2019).â€ Accessed on April 10, 2020. <a href=\"https://trends.google.com/trends/explore?date=2016-04-10%202019-04-10&amp;q=AI%20Ethics\">https://trends.google.com/trends/explore?date=2016-04-10%202019-04-10&amp;q=AI%20Ethics</a>.</p><p>[6]: Google Trends, â€œPrivacy Software Search Interest (2016â€“2019).â€ Accessed on April 10, 2020. <a href=\"https://trends.google.com/trends/explore?date=2016-04-10%202019-04-10&amp;q=Privacy%20Software\">https://trends.google.com/trends/explore?date=2016-04-10%202019-04-10&amp;q=Privacy%20Software</a>.</p><p>[7]: Google Trends, â€œAI and ML Search Interest (2016â€“2019).â€ Accessed on April 10, 2020. <a href=\"https://trends.google.com/trends/explore?date=2016-04-10%202019-04-10&amp;q=Machine%20Learning,%2Fm%2F0mkz\">https://trends.google.com/trends/explore?date=2016-04-10%202019-04-10&amp;q=Machine%20Learning,%2Fm%2F0mkz</a>.</p><p>[8]: <em>General Data Protection Regulation</em>, <em>European Parliament</em>2016, 1â€“77.</p><p>[9]: <em>Personal Information Protection and Electronic Documents Act</em>, <em>Revised Statutes of Canada</em> 2000, 4â€“39.</p><p>[10]: Bolukbasi, Tolga, Kai-Wei Chang, James Y. Zou, Venkatesh Saligrama, and Adam T. Kalai. â€œMan is to computer programmer as woman is to homemaker? debiasing word embeddings.â€ In <em>Advances in neural information processing systems</em>, pp. 4350. 2016.</p><p>[11]: Waldman, Ari Ezra. â€œPower, Process, and Automated Decision-Making.â€ <em>Fordham L. Rev.</em> 88 (2019): 613.</p><p>[12]: Kamarinou, Dimitra, Christopher Millard, and Jatinder Singh. â€œMachine Learning with Personal Data: Profiling, Decisions and the EU General Data Protection Regulation.â€ <em>Journal of Machine Learning Research</em>(2017): 1â€“7.</p><p>[13]: Golea, Mostefa, and Mario Marchand. â€œA growth algorithm for neural network decision trees.â€ <em>EPL (Europhysics Letters)</em> 12, no. 3 (1990): 205.</p><p>[14]: Kamarinou, Dimitra, Christopher Millard, and Jatinder Singh. â€œMachine Learning with Personal Data: Profiling, Decisions and the EU General Data Protection Regulation.â€ <em>Journal of Machine Learning Research</em>(2017): 1â€“7.</p><p>[15]: Shintre, Saurabh, Kevin A. Roundy, and Jasjeet Dhaliwal. â€œMaking Machine Learning Forget.â€ In <em>Annual Privacy Forum</em>, pp. 72â€“83. Springer, Cham, 2019.</p>"}