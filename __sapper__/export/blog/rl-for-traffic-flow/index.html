<!DOCTYPE html> <html lang=en> <head> <link href="https://fonts.googleapis.com/css?family=Mukta|Poppins:500,600&display=swap" rel=stylesheet> <meta charset=utf-8> <meta content="width=device-width,initial-scale=1" name=viewport> <meta content=#333333 name=theme-color> <base href=/ > <link href=global.css rel=stylesheet> <link href=manifest.json rel=manifest crossorigin=use-credentials> <link href=favicon.png rel=icon type=image/png> <link href=client/main.3898572404.css rel=stylesheet><link href=client/[slug].3f331e1e.css rel=stylesheet><link href=client/client.e75a9efc.css rel=stylesheet><link href=client/Tag.2ddad0a9.css rel=stylesheet> <noscript id=sapper-head-start></noscript><title>üöôReinforcement Learning for Traffic Flow - Cameron Raymondüöô</title><link href=https://cameronraymond.me/blog/rl-for-traffic-flow/ rel=canonical><meta content="A reinforcement learning agent that helps control the flow of traffic. Through this simple RL algorithm, we were able to reduce carbon emissions by a third, and cut time waiting at red lights in half." name=description><meta content="Cameron Raymond, University of Oxford, Oxford University, Data
    Science, Social Data Sience, Data Scientist" name=keywords><meta content=website property=og:type><meta content=https://cameronraymond.me/blog/rl-for-traffic-flow/ property=og:url><meta content="üöôReinforcement Learning for Traffic Flow - Cameron Raymondüöô" property=og:title><meta content="A reinforcement learning agent that helps control the flow of traffic. Through this simple RL algorithm, we were able to reduce carbon emissions by a third, and cut time waiting at red lights in half." name=og:description><meta content=https://cameronraymond.me/networkd.png property=og:image><meta content=summary property=twitter:card><meta content=https://cameronraymond.me/blog/rl-for-traffic-flow/ property=twitter:url><meta content="üöôReinforcement Learning for Traffic Flow - Cameron Raymondüöô" property=twitter:title><meta content="A reinforcement learning agent that helps control the flow of traffic. Through this simple RL algorithm, we were able to reduce carbon emissions by a third, and cut time waiting at red lights in half." property=twitter:description><meta content=https://cameronraymond.me/networkd.png property=twitter:image><noscript id=sapper-head-end></noscript> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-133541363-2"></script> <script> window.dataLayer = window.dataLayer || []
      function gtag() {
        dataLayer.push(arguments)
      }
      gtag('js', new Date())

      gtag('config', 'UA-133541363-2') </script> </head> <body> <div id=sapper> <div class="svelte-ex3z1i margin"></div> <form action=https://tinyletter.com/cjkraymond class="svelte-ex3z1i sign-up-banner" method=post onsubmit="window.open('https://tinyletter.com/cjkraymond', 'popupwindow',
  'scrollbars=yes,width=800,height=600');return true" target=popupwindow><p><label class="svelte-ex3z1i label" for=tlemail>Let's be pals:</label></p> <div class=signup-input><input class="svelte-ex3z1i email" name=email id=tlemail placeholder=my@email.com></div> <input class=svelte-ex3z1i name=embed type=hidden value=1> <span class="svelte-ex3z1i tooltip"><input class="svelte-ex3z1i subscribe" type=submit value=‚úâÔ∏è> <span class="svelte-ex3z1i tooltiptext">Subscribe!</span></span> <span class=svelte-ex3z1i id=close-sub><svg class=svelte-c8tyih viewBox="0 0 512 512" xmlns=http://www.w3.org/2000/svg><path d="M331.3 308.7L278.6 256l52.7-52.7c6.2-6.2 6.2-16.4 0-22.6-6.2-6.2-16.4-6.2-22.6 0L256 233.4l-52.7-52.7c-6.2-6.2-15.6-7.1-22.6 0-7.1 7.1-6 16.6 0 22.6l52.7 52.7-52.7 52.7c-6.7 6.7-6.4 16.3 0 22.6 6.4 6.4 16.4 6.2 22.6 0l52.7-52.7 52.7 52.7c6.2 6.2 16.4 6.2 22.6 0 6.3-6.2 6.3-16.4 0-22.6z"></path> <path d="M256 76c48.1 0 93.3 18.7 127.3 52.7S436 207.9 436 256s-18.7 93.3-52.7 127.3S304.1 436 256 436c-48.1 0-93.3-18.7-127.3-52.7S76 304.1 76 256s18.7-93.3 52.7-127.3S207.9 76 256 76m0-28C141.1 48 48 141.1 48 256s93.1 208 208 208 208-93.1 208-208S370.9 48 256 48z"></path></svg></span></form> <nav class=svelte-1otdzf2><a aria-label=Home href=. class=svelte-1otdzf2 rel=prefetch>Cameron Raymond </a> <div class="svelte-1otdzf2 links"><a aria-label=Resume href=cameron-raymond-resume.pdf class=svelte-1otdzf2 target=_blank><span class="svelte-1otdzf2 icon hideIcons"><svg class=svelte-c8tyih viewBox="0 0 576 512" xmlns=http://www.w3.org/2000/svg><path d="M552 64H88c-13.255 0-24 10.745-24 24v8H24c-13.255 0-24 10.745-24 24v272c0 30.928 25.072 56 56 56h472c26.51 0 48-21.49 48-48V88c0-13.255-10.745-24-24-24zM56 400a8 8 0 0 1-8-8V144h16v248a8 8 0 0 1-8 8zm236-16H140c-6.627 0-12-5.373-12-12v-8c0-6.627 5.373-12 12-12h152c6.627 0 12 5.373 12 12v8c0 6.627-5.373 12-12 12zm208 0H348c-6.627 0-12-5.373-12-12v-8c0-6.627 5.373-12 12-12h152c6.627 0 12 5.373 12 12v8c0 6.627-5.373 12-12 12zm-208-96H140c-6.627 0-12-5.373-12-12v-8c0-6.627 5.373-12 12-12h152c6.627 0 12 5.373 12 12v8c0 6.627-5.373 12-12 12zm208 0H348c-6.627 0-12-5.373-12-12v-8c0-6.627 5.373-12 12-12h152c6.627 0 12 5.373 12 12v8c0 6.627-5.373 12-12 12zm0-96H140c-6.627 0-12-5.373-12-12v-40c0-6.627 5.373-12 12-12h360c6.627 0 12 5.373 12 12v40c0 6.627-5.373 12-12 12z"></path></svg></span> <span class="svelte-1otdzf2 hideLinks">Resume</span></a> <a aria-label=Blog href=blog/ class=svelte-1otdzf2 rel=prefetch aria-current=page><span class="svelte-1otdzf2 icon hideIcons"><svg class=svelte-c8tyih viewBox="0 0 512 512" xmlns=http://www.w3.org/2000/svg><path d="M497.9 142.1l-46.1 46.1c-4.7 4.7-12.3 4.7-17 0l-111-111c-4.7-4.7-4.7-12.3 0-17l46.1-46.1c18.7-18.7 49.1-18.7 67.9 0l60.1 60.1c18.8 18.7 18.8 49.1 0 67.9zM284.2 99.8L21.6 362.4.4 483.9c-2.9 16.4 11.4 30.6 27.8 27.8l121.5-21.3 262.6-262.6c4.7-4.7 4.7-12.3 0-17l-111-111c-4.8-4.7-12.4-4.7-17.1 0zM124.1 339.9c-5.5-5.5-5.5-14.3 0-19.8l154-154c5.5-5.5 14.3-5.5 19.8 0s5.5 14.3 0 19.8l-154 154c-5.5 5.5-14.3 5.5-19.8 0zM88 424h48v36.3l-64.5 11.3-31.1-31.1L51.7 376H88v48z"></path></svg></span> <span class="svelte-1otdzf2 hideLinks">Blog</span></a> <a aria-label=About href=about/ class=svelte-1otdzf2 rel=prefetch><span class="svelte-1otdzf2 icon hideIcons"><svg class=svelte-c8tyih viewBox="0 0 448 512" xmlns=http://www.w3.org/2000/svg><path d="M224 256c70.7 0 128-57.3 128-128S294.7 0 224 0 96 57.3 96 128s57.3 128 128 128zm89.6 32h-16.7c-22.2 10.2-46.9 16-72.9 16s-50.6-5.8-72.9-16h-16.7C60.2 288 0 348.2 0 422.4V464c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48v-41.6c0-74.2-60.2-134.4-134.4-134.4z"></path></svg></span> <span class="svelte-1otdzf2 hideLinks">About</span></a> <div class="svelte-1otdzf2 divider"></div> <a aria-label=LinkedIn href=https://www.linkedin.com/in/CJKRaymond/ class="svelte-1otdzf2 icon"><svg class=svelte-c8tyih viewBox="0 0 448 512" xmlns=http://www.w3.org/2000/svg><path d="M100.28 448H7.4V148.9h92.88zM53.79 108.1C24.09 108.1 0 83.5 0 53.8a53.79 53.79 0 0 1 107.58 0c0 29.7-24.1 54.3-53.79 54.3zM447.9 448h-92.68V302.4c0-34.7-.7-79.2-48.29-79.2-48.29 0-55.69 37.7-55.69 76.7V448h-92.78V148.9h89.08v40.8h1.3c12.4-23.5 42.69-48.3 87.88-48.3 94 0 111.28 61.9 111.28 142.3V448z"></path></svg></a> <a aria-label=twitter href=https://twitter.com/CJKRaymond class="svelte-1otdzf2 icon"><svg class=svelte-c8tyih viewBox="0 0 512 512" xmlns=http://www.w3.org/2000/svg><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a></div></nav> <main class=svelte-y41jpk> <div></div> </main> <div class="svelte-16irf4m chat"><div></div></div> <div class="svelte-16irf4m footer"><p class=svelte-16irf4m><span aria-hidden=false aria-label=Emoji role=img>üë®‚Äçüé®</span> + <span aria-hidden=false aria-label=Emoji role=img>üë∑‚Äç‚ôÇÔ∏è</span> by me <br> Last updated <a aria-label="April
      06, 21" href=https://www.onthisday.com/events/April/06>April 06, '21</a></p> <div class="svelte-16irf4m links"><a aria-label=LinkedIn href=https://www.linkedin.com/in/CJKRaymond/ class="svelte-16irf4m icon"><svg class=svelte-c8tyih viewBox="0 0 448 512" xmlns=http://www.w3.org/2000/svg><path d="M100.28 448H7.4V148.9h92.88zM53.79 108.1C24.09 108.1 0 83.5 0 53.8a53.79 53.79 0 0 1 107.58 0c0 29.7-24.1 54.3-53.79 54.3zM447.9 448h-92.68V302.4c0-34.7-.7-79.2-48.29-79.2-48.29 0-55.69 37.7-55.69 76.7V448h-92.78V148.9h89.08v40.8h1.3c12.4-23.5 42.69-48.3 87.88-48.3 94 0 111.28 61.9 111.28 142.3V448z"></path></svg></a> <div class="svelte-16irf4m divider"></div> <a aria-label=twitter href=https://twitter.com/CJKRaymond class="svelte-16irf4m icon"><svg class=svelte-c8tyih viewBox="0 0 512 512" xmlns=http://www.w3.org/2000/svg><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a> <div class="svelte-16irf4m divider"></div> <a aria-label=Github href=https://github.com/cameron-raymond/ class="svelte-16irf4m icon"><svg class=svelte-c8tyih viewBox="0 0 496 512" xmlns=http://www.w3.org/2000/svg><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a> <div class="svelte-16irf4m divider"></div> <a aria-label=Medium href=https://medium.com/@cameronraymond/ class="svelte-16irf4m icon"><svg class=svelte-c8tyih viewBox="0 0 512 512" xmlns=http://www.w3.org/2000/svg><path d="M71.5 142.3c.6-5.9-1.7-11.8-6.1-15.8L20.3 72.1V64h140.2l108.4 237.7L364.2 64h133.7v8.1l-38.6 37c-3.3 2.5-5 6.7-4.3 10.8v272c-.7 4.1 1 8.3 4.3 10.8l37.7 37v8.1H307.3v-8.1l39.1-37.9c3.8-3.8 3.8-5 3.8-10.8V171.2L241.5 447.1h-14.7L100.4 171.2v184.9c-1.1 7.8 1.5 15.6 7 21.2l50.8 61.6v8.1h-144v-8L65 377.3c5.4-5.6 7.9-13.5 6.5-21.2V142.3z"></path></svg></a></div></div> <a aria-label=" " href=blog/ class=svelte-1i4fw52>blog</a> <a aria-label=" " href=sitemap.xml class=svelte-1i4fw52>sitemap</a></div> <script>__SAPPER__={baseUrl:"",preloaded:[void 0,null,{post:{title:"Reinforcement Learning for Traffic Flow",slug:"rl-for-traffic-flow",emoji:"üöô",blurb:"A reinforcement learning agent that helps control the flow of traffic. Through this simple RL algorithm, we were able to reduce carbon emissions by a third, and cut time waiting at red lights in half.",type:"sp",tags:["rl"],collaborators:["rosslh","hughcorley","zanelittle","leonard-zhao"],link:"\u003Ca aria-label='Repo' href='https:\u002F\u002Fgithub.com\u002FZaneLittle\u002FTraffic-Light-Simulation##q-learning-for-traffic-signal-control'\u003ERepo\u003C\u002Fa\u003E",date:"Dec. 1, 2019",prod:true,html:"\u003Ch2 id=\"introduction\"\u003EIntroduction\u003C\u002Fh2\u003E\n\u003Ch3 id=\"motivation\"\u003EMotivation\u003C\u002Fh3\u003E\n\u003Cimg src=\"content\u002Frl-for-traffic-flow\u002Fshowcase.gif\" alt=\"alt_text\" title=\"Figure 1: Illustration of a road network with overlay of the queues modelled at each intersection\"\u003E\n\u003Cem\u003EFigure 1: Trained RL agent coordinating rush hour traffic.\u003C\u002Fem\u003E\n\u003Cp\u003EBefore undertaking this project, we had several ideas of what problem to undertake, including solving games such as Snake or Space Invaders. However, many traditional games have already been solved using reinforcement learning methods. While this project takes inspiration from previous studies on using reinforcement learning techniques for traffic light control, the exact nature of the environment (which we have created from scratch) can be easily changed and thus present an entirely new problem. Additionally, although problems such as Snake are interesting, they have no real-world applications other than entertainment. Conversely, traffic light control systems have immense importance in the real-world; poorly designed systems impact commute times and increase carbon emissions due to congestion.\u003C\u002Fp\u003E\u003Ch3 id=\"relevance\"\u003ERelevance\u003C\u002Fh3\u003E\n\u003Cp\u003EOur project involves optimizing traffic light controls. This is a real-life problem that is especially pertinent in congested cities. Inefficient traffic control systems are costly in many ways. While people and their vehicles are being stuck in traffic, their time is being wasted (usually at crucial times of the day, such as rush-hour), expensive fuel is being consumed, and harmful greenhouse gases are emitted. Great lengths have been taken to mitigate some of these effects (for example, auto stop-start systems have been a recent development); however, there is much work to be done. One obvious approach to reduce traffic congestion in the first place is by designing more efficient traffic control systems.\u003C\u002Fp\u003E\u003Cp\u003EThroughout this project we have attempted to design a reinforcement learning agent which controls traffic lights in a simulated environment. The goal is to minimize the expected wait time for cars within the environment, which is composed of four different intersections, each of which controls traffic using traffic lights.\u003C\u002Fp\u003E\u003Ch2 id=\"problem-formulation\"\u003EProblem Formulation\u003C\u002Fh2\u003E\n\u003Ch3 id=\"environment\"\u003EEnvironment\u003C\u002Fh3\u003E\n\u003Cimg src=\"content\u002Frl-for-traffic-flow\u002Ftraffic-setup.png\" alt=\"alt_text\" title=\"Figure 1: Illustration of a road network with overlay of the queues modelled at each intersection\"\u003E\n\u003Cem\u003EFigure 2: Illustration of a road network with overlay of the queues modeled at each intersection\u003C\u002Fem\u003E\n\u003Cp\u003EThe objective we want to optimize is minimizing the average wait time of cars. Thus, we want a policy for switching traffic light controls that will ensure a consistent and smooth transition of cars from one intersection to the next.\u003C\u002Fp\u003E\u003Cp\u003EWe attempt to model a road network such as \u003Cem\u003EFigure 2\u003C\u002Fem\u003E. In order to reflect this, we developed our own environment in simulation. It consists of four intersections, each with a traffic light entity that controls traffic at that intersection, along with 4 queues at each intersection that represent all the cars that are waiting to go through.\u003C\u002Fp\u003E\u003Cp\u003ECars are randomly added to one of the queues on the ‚Äúoutside‚Äù of the network (such as the southern facing queue of the north-west traffic light). As time progresses, cars drive a randomized, predetermined route through the network, and out an exit node on the outside of the network. These optimal routes are determined, given a start and end node, by performing a breadth-first search. All cars in the same queue can be thought of as facing the same direction, though they don‚Äôt necessarily need to be heading the same way. If the light at the queue a car is in is green, they are popped from this queue and pushed to an adjacent queue, depending on the route they intend to take.\u003C\u002Fp\u003E\u003Cp\u003EWe discretize episode time-steps by considering each step as the time it takes a single car to move through the intersection. Thus, at each time step, every traffic light can change its lights, and up to one car from each queue can move through the intersection to either the next queue or to its exit location. We also consider some rules in our environment for simplification: cars can‚Äôt make left or right turns when they are facing a red light; cars take two time steps to move from one queue to another (this way cars are not ‚Äúwaiting‚Äù in the next queue until two time steps later) so as to simulate the time it would take to drive from one light to another; and cars are always able to make left turns when facing green lights.\u003C\u002Fp\u003E\u003Cp\u003ETo better simulate real-life traffic, we implement high-traffic and low-traffic times to recreate ‚Äúrush-hour‚Äù traffic. The difference is that during high-traffic times, more cars are being generated into the environment at each time step. Furthermore, we split the time steps into days (with 600 time steps per day) and years. The environment is reset at the end of each day, meaning that all cars still in the environment are removed. During each day, we have two high-traffic periods to simulate the standard two rush hours per day.\u003C\u002Fp\u003E\u003Ch3 id=\"state-space\"\u003EState space\u003C\u002Fh3\u003E\n\u003Cp\u003EEach intersection is controlled by a traffic light entity that has two states: allowing north-south traffic (light is green for cars north and south of the light and red for cars east and west of the light) or allowing east-west traffic. Since we are trying to minimize waiting times for cars, we also need to include waiting times into our state space. As it would be unrealistic to account for a continuous measure of wait times for every car, we instead discretize this for each queue of cars. The amount of time each car has been waiting in the north and south queues of a particular intersection is summed. This value is binned as a zero, low, medium, or high waiting time for the pair of queues. The same is done for the east and west queues for each intersection.\u003C\u002Fp\u003E\u003Cp\u003EThus we have a state space with a maximum of: 2\u003Csup\u003Elights\u003C\u002Fsup\u003E ‚ãÖ 4\u003Csup\u003E2‚ãÖlights\u003C\u002Fsup\u003E = 2\u003Csup\u003E4\u003C\u002Fsup\u003E ‚ãÖ 4\u003Csup\u003E8\u003C\u002Fsup\u003E = 1,048,576 states.\u003C\u002Fp\u003E\u003Ch3 id=\"action-space\"\u003EAction space\u003C\u002Fh3\u003E\n\u003Cp\u003EEach light can enable north-south traffic or east-west traffic. This means that there are two actions available for each light at each step: switch light directions (turn green lights to red or turn red lights to green) or do nothing. Thus, the total action space is 2\u003Csup\u003E4\u003C\u002Fsup\u003E = 16 possible actions.\u003C\u002Fp\u003E\u003Ch3 id=\"reward-scheme\"\u003EReward scheme\u003C\u002Fh3\u003E\n\u003Cp\u003EAs the objective is to minimize average waiting times, in order to maximize the reward we give a negative value for wait times. For each queue, we obtain the sum of the wait times for all the cars in the queue, and this is categorized as one of four bins. There is a bin for zero wait time, which results in zero cost. The next lowest bin (‚Äúlow total wait time‚Äù) gives a small negative reward (low cost) and the highest bin (‚Äúhigh total wait time‚Äù) gives the large negative reward (most cost). A light ‚Äúqueue‚Äù that either has no cars in it, or is green in that direction, does not contribute to the reward.\u003C\u002Fp\u003E\u003Ch2 id=\"solution-overview\"\u003ESolution Overview\u003C\u002Fh2\u003E\n\u003Ch3 id=\"approach\"\u003EApproach\u003C\u002Fh3\u003E\n\u003Cp\u003EWe used Q-learning for training the agent. The Q-table is represented as a dictionary with the state value being the key, and its value being the A(s). By using a dictionary (a hash table) instead of an array, we were able to avoid storing entries until they were visited. This reduced memory usage as, in-practice, only 8-12% of the state space needed to be explored before convergence. We use a learning rate, \u003Cem\u003EŒ±\u003C\u002Fem\u003E, of 0.9; a discount factor, \u003Cem\u003EŒ≥\u003C\u002Fem\u003E, of 0.5; and an exploration rate, \u003Cem\u003EŒµ\u003C\u002Fem\u003E of 0.01. Additionally, we implemented a softmax policy to compare against the \u003Cem\u003EŒµ\u003C\u002Fem\u003E-greedy policy. We decided to use Q-learning as we have a very large state space so bootstrapping would be preferable to keep training times reasonable.\u003C\u002Fp\u003E\u003Ch3 id=\"environment-1\"\u003EEnvironment\u003C\u002Fh3\u003E\n\u003Cp\u003ENo external environment, simulation or data sets were used. The agent was trained exclusively on the aforementioned environment we created.\u003C\u002Fp\u003E\u003Ch2 id=\"results\"\u003EResults\u003C\u002Fh2\u003E\n\u003Ch3 id=\"comparison-of-softmax-and-Œµ-greedy-for-a-normal-and-loop-route\"\u003EComparison of softmax and \u003Cem\u003EŒµ\u003C\u002Fem\u003E-greedy for a normal and loop route\u003C\u002Fh3\u003E\n\u003Cp\u003EWe first compared the performance of using an \u003Cem\u003EŒµ\u003C\u002Fem\u003E-greedy and a softmax policy with our ‚Äúnormal‚Äù route generation, or simply optimal routes with randomized start and end points. We can see from Figure 2 and Figure 3 that both models have similar performances, though the \u003Cem\u003EŒµ\u003C\u002Fem\u003E-greedy model converged to a slightly lower average wait time.\u003C\u002Fp\u003E\u003Cimg src=\"content\u002Frl-for-traffic-flow\u002FDailyAvg_NormalRoute_Softmax.png\" alt=\"alt_text\" title=\"Figure 2: Softmax daily averages for normal route.\"\u003E\n\u003Cem\u003EFigure 3: Softmax daily averages for normal route.\u003C\u002Fem\u003E\n\u003Cimg src=\"content\u002Frl-for-traffic-flow\u002FDailyAvg_NormalRoute_EGreedy.png\" alt=\"alt_text\" title=\"Figure 3: Œµ-greedy daily averages for normal route.\"\u003E\n\u003Cem\u003EFigure 4: Œµ-greedy daily averages for normal route.\u003C\u002Fem\u003E\n\u003Cp\u003EWe then compared the two policies using a ‚Äúloop‚Äù route, where every car performs 10 counter clockwise loops and then exits.\u003C\u002Fp\u003E\u003Cimg src=\"content\u002Frl-for-traffic-flow\u002FDailyAvg_LoopRoute_Softmax.png\" alt=\"alt_text\" title=\"Figure 4: Softmax daily averages for looping route.\"\u003E\n\u003Cem\u003EFigure 5: Softmax daily averages for looping route.\u003C\u002Fem\u003E\n\u003Cimg src=\"content\u002Frl-for-traffic-flow\u002FDailyAvg_LoopRoute_EGreedy.png\" alt=\"alt_text\" title=\"Figure 5: Œµ-greedy daily averages for looping route.\"\u003E\n\u003Cem\u003EFigure 6: Œµ-greedy daily averages for looping route.\u003C\u002Fem\u003E\n\u003Cp\u003EFrom Figure 4 and Figure 5 above, we can see that using a softmax policy on the loop route enabled the agent to learn the loop. Comparing this to Figure 5, the \u003Cem\u003EŒµ\u003C\u002Fem\u003E-greedy agent converges but has some difficulty as time goes on. Due to the difference in performance between the two policies when using ‚Äúrandom‚Äù routes and looped routes, the choice of which policy to use would depend on how the cars behave in the environment.\u003C\u002Fp\u003E\u003Cp\u003EThe trained agent is able to efficiently control traffic in a robust manner. Despite having a great variation in the number of cars added to the environment (for example, during rush hour), the time cost per car stays relatively constant, as shown in Figure 6 below.\u003C\u002Fp\u003E\u003Cimg src=\"content\u002Frl-for-traffic-flow\u002FOneDay_NormalRoute_EGreedy.png\" alt=\"alt_text\" title=\"Figure 6: Cost per car throughout a day. Note the two periods of rush hour in the lower plot.\"\u003E\n\u003Cem\u003EFigure 7: Cost per car throughout a day. Note the two periods of rush hour in the lower plot.\u003C\u002Fem\u003E\n\u003Cp\u003EIn addition to modeling the movement of vehicles throughout the environment, the amount of carbon dioxide produced by each vehicle was also simulated. As is expected, utilization of the learning agent resulted not only in more efficient traffic control, but also resulted in less carbon dioxide being produced each day. The results of certain routes (either normal or loop) and policy being used (either softmax or \u003Cem\u003EŒµ\u003C\u002Fem\u003E-greedy) are similar to that of the results for the wait times discussed previously, with softmax performing better for the loop route, and \u003Cem\u003EŒµ\u003C\u002Fem\u003E-greedy performing better for the normal route.\u003C\u002Fp\u003E\u003Cimg src=\"content\u002Frl-for-traffic-flow\u002FCO2_NormalRoute_Softmax.png\" alt=\"alt_text\" title=\"Figure 7: Cumulative CO2 for normal route using softmax.\"\u003E\n\u003Cem\u003EFigure 8: Cumulative CO2 for normal route using softmax.\u003C\u002Fem\u003E\n\u003Cimg src=\"content\u002Frl-for-traffic-flow\u002FCO2_NormalRoute_EGreedy.png\" alt=\"alt_text\" title=\"Figure 8: Cumulative CO2 for normal route using Œµ-greedy.\"\u003E\n\u003Cem\u003EFigure 9: Cumulative CO2 for normal route using Œµ-greedy.\u003C\u002Fem\u003E\n\u003Cimg src=\"content\u002Frl-for-traffic-flow\u002FCO2_LoopRoute_Softmax.png\" alt=\"alt_text\" title=\"Figure 9: Cumulative CO2 for loop route using softmax.\"\u003E\n\u003Cem\u003EFigure 10: Cumulative CO2 for loop route using softmax.\u003C\u002Fem\u003E\n\u003Cimg src=\"content\u002Frl-for-traffic-flow\u002FCO2_LoopRoute_EGreedy.png\" alt=\"alt_text\" title=\"Figure 10: Cumulative CO2 for loop route using Œµ-greedy.\"\u003E\n\u003Cem\u003EFigure 11: Cumulative CO2 for loop route using Œµ-greedy.\u003C\u002Fem\u003E\n\u003Ch2 id=\"conclusion\"\u003EConclusion\u003C\u002Fh2\u003E\n\u003Cp\u003EA number of adjustments to our solution had to be made during the development of our agent in order to achieve desirable results. Reducing the state space of the problem is key in decreasing training time. Initially, our approach involved including the current time of the simulation in the state, so that the agent would be able to learn about when rush-hour occurs and adjust its strategy accordingly. Ultimately we learned that including time in our state greatly increases the size of the state space, and that the agent ‚Äúlearns‚Äù how to handle rush hour through the information embedded in the queues - their wait times. Therefore, this extraneous piece of information was not necessary.\u003C\u002Fp\u003E\u003Cp\u003EWe learned that, given problems with different parameters, soft-max or e-greedy can be the better approach, and the developer has to apply a different strategy based on the circumstances. Experimenting with these policies revealed how using different parameters would change our results and potentially improve our model for specific situations.\u003C\u002Fp\u003E\u003Cp\u003EWe also learned that decreasing the size of the state-space doesn‚Äôt necessarily provide a performance gain; by adding a bin for zero wait time, the agent was able to differentiate between queues with \u003Cem\u003Efew\u003C\u002Fem\u003E cars and queues with \u003Cem\u003Eno\u003C\u002Fem\u003E cars, decreasing travel times.\u003C\u002Fp\u003E\u003Cp\u003EFinally, working on a problem tied to a real-life application helps us see how reinforcement learning can be applied to improve real-world systems. Over the course of a 140 day run, our system was able to reduce carbon emissions by approximately 2000 kg over a naive agent for a car with a deterministic route. Our solution can be extended very easily to the real-world not only to reduce our carbon footprint, but also to save travel time for the everyday driver.\u003C\u002Fp\u003E"}}]};if('serviceWorker' in navigator)navigator.serviceWorker.register('/service-worker.js');(function(){try{eval("async function x(){}");var main="/client/client.e75a9efc.js"}catch(e){main="/client/legacy/client.e53be589.js"};var s=document.createElement("script");try{new Function("if(0)import('')")();s.src=main;s.type="module";s.crossOrigin="use-credentials";}catch(e){s.src="/client/shimport@1.0.1.js";s.setAttribute("data-main",main);}document.head.appendChild(s);}());</script> 